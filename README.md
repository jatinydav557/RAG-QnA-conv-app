
---

````markdown
# ğŸ§¾ Conversational RAG PDF Q&A App | LLMs + Memory + Chroma

This project allows you to **chat with your PDFs** using a Retrieval-Augmented Generation (RAG) pipeline.  
Built using **LangChain**, **Groq's Gemma2-9B**, **HuggingFace Embeddings**, and **Chroma DB**, the app supports real-time conversation with memory using uploaded research papers.

ğŸ§  Powered by `RunnableWithMessageHistory` and context-aware question reformulation.

---

## ğŸ“Œ Use Case

Upload a paper like _â€œSelf-Attention for Generative Modelsâ€_ and interactively ask questions while preserving chat memory â€” similar to ChatGPT browsing mode, but local and focused on your documents.

---

## ğŸ§© Stack Overview

- ğŸ”¥ **LLM**: Groqâ€™s `gemma2-9b-it`
- ğŸ§  **Embeddings**: HuggingFace `all-MiniLM-L6-v2`
- ğŸ“¦ **Vector Store**: ChromaDB
- ğŸ§  **LangChain Features**: Message History, Chaining, Memory-aware Retrieval
- ğŸ’¬ **Frontend**: Streamlit

---

## ğŸ“ Folder Structure

```bash
.
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ .env                  # Secrets like GROQ key, HF token
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ temp.pdf              # Placeholder for uploaded files
â””â”€â”€ README.md             # You're reading it
````

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/yourusername/conversational-rag-pdf.git
cd conversational-rag-pdf
```

### 2ï¸âƒ£ Set up Virtual Environment

```bash
uv venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Your `.env` File

```env
HF_TOKEN=your_huggingface_token
```

Groq API key is entered inside the Streamlit UI at runtime.

---

## ğŸš€ Running the App

```bash
streamlit run app.py
```

---

## ğŸ› ï¸ Example Workflow

1. ğŸ”‘ Enter your **Groq API key**
2. ğŸ“„ Upload PDFs (e.g., *Self-Attention for Generative Models*)
3. ğŸ—£ï¸ Ask questions using natural language
4. ğŸ§  App reformulates questions based on history
5. ğŸ’¬ Get contextual answers + chat history

---

## ğŸ§ª Dependencies

```
streamlit
python-dotenv
langchain
langchain-core
langchain-community
langchain-groq
langchain-ollama
langchain-huggingface
langchain-chroma
chromadb
huggingface-hub
faiss-cpu
pypdf
protobuf==3.20.3
```

---

## ğŸ“ About Me

I'm an **MCA final-year student**, exploring **Applied AI, Generative Models, and LLM RAG systems**.
This project reflects my learning in memory-aware chat systems, embeddings, and real-time vector search.

ğŸ“Œ Open to internships and full-time roles in:

* ğŸ“Š Data Science
* ğŸ¤– LLM Engineering
* ğŸ§  NLP / RAG Systems
* âš™ï¸ MLOps / AI Research Engineering

ğŸ”— [LinkedIn](https://www.linkedin.com/in/yourname)
ğŸŒ [Portfolio](https://yourwebsite.com)

---

## ğŸ§­ Future Plans

* ğŸŒ Deploy on Streamlit Cloud / Hugging Face Spaces
* ğŸ’¾ Save chat history per user to database
* ğŸ”— Support multi-PDF chaining and chunk summarization

---

â­ **Star this repo** if it helped you! Fork it to build your own research Q\&A app!

```

---

### âœ… What to Replace:
- `yourusername` in clone URL
- `your_huggingface_token` in `.env`
- LinkedIn and portfolio URLs

Let me know if you'd like:
- A banner for this project (like the others)
- A Hugging Face Spaces deployment guide
- Streamlit Cloud or GCP deployment workflow

Want me to generate a banner for this too?
```
